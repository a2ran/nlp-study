{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bf70d5",
   "metadata": {},
   "source": [
    "# Stanford CS224N: Lecture 2 - Neural Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e228487",
   "metadata": {},
   "source": [
    "# <span style = \"color: blue\"> Lecture (강의 내용) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8ce5f",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff1069b",
   "metadata": {},
   "source": [
    "$\\theta^{new} = \\theta^{old} - \\alpha\\nabla_{\\theta}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8d016",
   "metadata": {},
   "source": [
    "**Gradient Descent** calculates the gradient (slope) of the cost function, <br> \n",
    "which is the error between the *predicted value* and *the actual value*, <br>\n",
    "and updates $\\theta$ to negative gradient to reach the minimum value.\n",
    "\n",
    "However, since **Gradient Descent** calculates from a whole *Dataset*, <br>\n",
    "It costs too much memory and is time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645a96f",
   "metadata": {},
   "source": [
    "$\\nabla_{\\theta}J(\\theta)$ = $\\left[\\begin{array}{clr} 0 \\\\ ... \\\\ \\nabla_{t} \\\\ 0 \\\\ \\nabla_{\\theta} \\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d04c1",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent (SGD)** solves this problem by <br>\n",
    "updating the model parameters by using only **one randomly selected training sample** <br>\n",
    "in each iteration instead of using a whole *Dataset*.\n",
    "\n",
    "However, **SGD** method also suffers similar problem from **sparsity** <br>\n",
    "since it derives gradient using **one-hot vectors** which requires a plethora of *zero vectors*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40894059",
   "metadata": {},
   "source": [
    "**Skip-gram** uses a **Negative Sampling** method to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4969eb",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c85704",
   "metadata": {},
   "source": [
    "<img src = '1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea342b",
   "metadata": {},
   "source": [
    "**Negative Sampling** trains *binary logistic regressions* for a true pair versus several *noise pairs*.\n",
    "\n",
    "Negative Sampling inputs both *center word* and *context word*, <br>\n",
    "and predicts a **probability** of these two words actually present <br>\n",
    "in a certain window size.\n",
    "\n",
    "After making several predictions for center word and context words, <br>\n",
    "The model updates from the error between the *predicted value* and *the actual value* <br>\n",
    "using the **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff51a4",
   "metadata": {},
   "source": [
    "## Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5c852",
   "metadata": {},
   "source": [
    "### SVD (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22fc2f",
   "metadata": {},
   "source": [
    "<img src = '2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61293803",
   "metadata": {},
   "source": [
    "$A = U\\Sigma{V}^T$ <br>\n",
    "\n",
    "$A : m x n$ rectangular matrix <br>\n",
    "$U : m x m$ orthogonal matrix <br>\n",
    "$\\Sigma : m x n$ diagonal matrix <br>\n",
    "$V : n x n$ orthogonal matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2aceda",
   "metadata": {},
   "source": [
    "**Singular Value Decomposition** reduces the dimensionality of the data but preserves the most important aspects of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c77866",
   "metadata": {},
   "source": [
    "1. Compute the **left** and **right** **eigenvectors** of matric $A$, which is $U_k$ and $V_k$.\n",
    "2. The singular values of A are the non-negative square roots of the eigenvalues of the matrix $A^{T}A$. Arrange them at $\\Sigma$.\n",
    "3. The product of $U$, $\\Sigma$, $V$ is the original matrix $A$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
